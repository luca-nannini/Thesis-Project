{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Tweets of the First Debate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import and visualize the dataframe\n",
    "tweets = pd.read_csv(r'C:\\Users\\Luca Nannini\\Desktop/TweetsClean.csv')\n",
    "tweets.set_index(\"created_at\", inplace= True)\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reiterate over columns for visualize their names \n",
    "for col in tweets: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filtering only original tweets, removing of Retweets messages\n",
    "indexNames = tweets[tweets['RT'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_list = list(indexNames.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#optional: save the file while editing\n",
    "with open('tweet_list.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for item in tweet_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = ''.join(tweet_list)\n",
    "type(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of hashtags/account\n",
    "tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "#removal of punctuation\n",
    "tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\_\\=\\*\\@\\#\\$\\\"\\''\\``]\", \" \", tweet).split())\n",
    "#remove hyperlinks\n",
    "tweet = ' '.join(re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet).split())\n",
    "#remove stock market tickers like $GE\n",
    "tweet = ' '.join(re.sub(\"r'\\$\\W+\\w*\", \" \", tweet).split())\n",
    "#replace consecutive non-ASCII characters with a space\n",
    "tweet = ' '.join(re.sub(r\"[^\\x00-\\x7F]+\", \" \", tweet).split())\n",
    "#removal of numbers\n",
    "tweet = ' '.join(re.sub(r\"(\\d+)\", \" \", tweet).split())\n",
    "#removal chains of the same character\n",
    "tweet = ' '.join(re.sub(r'(.)\\1{2,}', r'\\1', tweet).split())\n",
    "tweet = tweet.lower()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang():\n",
    "    \n",
    "    return {\"'s\":\"is\",\n",
    "        \"s\":\"is\",\n",
    "        \"'re\":\"are\",\n",
    "        \"r\":\" are\", \n",
    "        \"'ll\":\"will\",\n",
    "        \"ll\":\"will\", \n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"i'm\":\"I am\",\n",
    "        \"im\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"rn\":\"right now\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wtf\":\"what the fuck\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLANG = slang()\n",
    "twtext = tweet.replace(\"â€™\",\"'\")\n",
    "words = twtext.split()\n",
    "reformed = [SLANG[word] if word in SLANG else word for word in words]\n",
    "TW = \" \".join(reformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW = ''.join(TW)\n",
    "type(TW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenize\n",
    "twtokens = nltk.word_tokenize(TW)\n",
    "#remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "twtokens = [token for token in twtokens if token not in stop]\n",
    "#remove words less than two letters\n",
    "twtokens = [word for word in twtokens if len(word) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "twtokens = [lmtzr.lemmatize(word, pos=\"v\") for word in twtokens]\n",
    "twtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweets = ' '.join(twtokens)\n",
    "preprocessed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweetstextcleaned.txt','w') as f:\n",
    "    f.write(preprocessed_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(twtokens)\n",
    "freq_words_tw = freq.most_common(50)\n",
    "freq_words_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sent_tweets = nltk.pos_tag(twtokens)\n",
    "print(tagged_sent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns_tweets= []\n",
    "for word, pos in tagged_sent_tweets:\n",
    "    if pos in ['NN', 'NNP', 'NNS', 'NNPS'] :\n",
    "        all_nouns_tweets.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tweets_nouns = nltk.FreqDist(all_nouns_tweets)\n",
    "top_common_nouns_tweets = freq_tweets_nouns.most_common(20)\n",
    "top_common_nouns_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verbs_tweets= []\n",
    "for word, pos in tagged_sent_tweets:\n",
    "    if pos in ['VB', 'VBG', 'VBN', 'VBP','VBZ']:\n",
    "        all_verbs_tweets.append(word)\n",
    "print(all_verbs_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distribution and top 20 verbs \n",
    "freq_tweets_verbs = nltk.FreqDist(all_verbs_tweets)\n",
    "top_common_verbs_tweets = freq_tweets_verbs.most_common(20)\n",
    "top_common_verbs_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Presidential Debate cycle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the dataframe and read it with candidates as index\n",
    "df = pd.read_csv(r'C:\\Users\\Luca Nannini\\Desktop/AllDebates.csv')\n",
    "df.set_index(\"CANDIDATE\", inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the floats, i.e. the missing values\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()\n",
    "#extract the TEXT columns as list\n",
    "text_list = list(df.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using list comprehensions, lowercase the list\n",
    "text_list = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert text from list to string\n",
    "text = ''.join(text_list)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw count of the word frequency for every item in the string\n",
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "    return counts\n",
    "print(word_count(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "# remove words less than two letters\n",
    "tokens = [word for word in tokens if len(word) >= 2]\n",
    "\n",
    "preprocessed_text= ' '.join(tokens)\n",
    "preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now visualize the 10 most frequent words\n",
    "freq_dist = nltk.FreqDist(tokens)\n",
    "freq_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sent = nltk.pos_tag(tokens)\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nouns= []\n",
    "for word, pos in tagged_sent:\n",
    "    if pos in ['NN', 'NNP', 'NNS', 'NNPS'] :\n",
    "        all_nouns.append(word)     \n",
    "print(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(word) for word in all_nouns]\n",
    "all_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_all_nouns = nltk.FreqDist(all_nouns)\n",
    "top_common_nouns = freq_all_nouns.most_common(50)\n",
    "top_common_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Candidate speeches in the first debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Luca Nannini\\Desktop/AllDebates.csv')\n",
    "df.set_index(\"CANDIDATE\", inplace= True)\n",
    "first_debate = df.iloc[:131]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#n.b. dataframe is imported again since the .drop() function previously performed has removed TRUMP from the index\n",
    "df = pd.read_csv(r'C:\\Users\\Luca Nannini\\Desktop/AllDebates.csv')\n",
    "df.set_index(\"CANDIDATE\", inplace= True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trump = first_debate.loc[\"TRUMP\", \"TEXT\"]\n",
    "trump_list = trump.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#starting from a lowercase list, tokenize words and remove stopwords for trump\n",
    "trump = ''.join(trump_list)\n",
    "trump = (re.sub(\"[^A-Za-z']+\", ' ', str(trump)).lower())\n",
    "trump_tokens = nltk.word_tokenize(trump)\n",
    "stop = stopwords.words('english')\n",
    "trump_tokens = [token for token in trump_tokens if token not in stop]\n",
    "trump_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words less than two letters\n",
    "trump_tokens = [word for word in trump_tokens if len(word) >= 2]\n",
    "# output a string for the cleaned tokens\n",
    "preprocessed_text= ' '.join(trump_tokens)\n",
    "#let's now visualize the 10 most frequent words without stopwords\n",
    "freq_trump_dist = nltk.FreqDist(trump_tokens)\n",
    "freq_trump_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#POS_tag\n",
    "tagged_sent_trump = nltk.pos_tag(trump_tokens)\n",
    "print(tagged_sent_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filtering only nouns\n",
    "all_nouns_trump= []\n",
    "for word, pos in tagged_sent_trump:\n",
    "    if pos in ['NN', 'NNP', 'NNS', 'NNPS']:\n",
    "        all_nouns_trump.append(word)\n",
    "print(all_nouns_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(word) for word in all_nouns_trump]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distribution and top 20 nouns \n",
    "freq_trump_nouns = nltk.FreqDist(tokens)\n",
    "top_common_nouns_trump = freq_trump_nouns.most_common(20)\n",
    "top_common_nouns_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filtering only verbs\n",
    "all_verbs_trump= []\n",
    "for word, pos in tagged_sent_trump:\n",
    "    if pos in ['VB', 'VBG', 'VBN', 'VBP','VBZ'] :\n",
    "        all_verbs_trump.append(word)\n",
    "print(all_verbs_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "lmtzr = WordNetLemmatizer()\n",
    "token_trump_verb = [lmtzr.lemmatize(word) for word in all_verbs_trump]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#frequency distribution and top 20 nouns \n",
    "freq_trump_verbs = nltk.FreqDist(token_trump_verb)\n",
    "top_common_verbs_trump = freq_trump_verbs.most_common(20)\n",
    "top_common_verbs_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clinton = first_debate.loc[\"CLINTON\", \"TEXT\"]\n",
    "clinton_list = clinton.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#starting from a lowercase list, tokenize words and remove stopwords for trump\n",
    "clinton = ''.join(clinton_list)\n",
    "clinton = (re.sub(\"[^A-Za-z']+\", ' ', str(clinton)).lower())\n",
    "\n",
    "clinton_tokens = nltk.word_tokenize(clinton)\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "clinton_tokens = [token for token in clinton_tokens if token not in stop]\n",
    "clinton_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove words less than two letters\n",
    "clinton_tokens = [word for word in clinton_tokens if len(word) >= 2]\n",
    "# output a string for the cleaned tokens\n",
    "preprocessed_text= ' '.join(clinton_tokens)\n",
    "#let's now visualize the 10 most frequent words without stopwords\n",
    "freq_dist = nltk.FreqDist(clinton_tokens)\n",
    "freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS_tag\n",
    "tagged_sent_clinton = nltk.pos_tag(clinton_tokens)\n",
    "print(tagged_sent_clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filtering only nouns\n",
    "all_nouns_clinton = []\n",
    "for word, pos in tagged_sent_clinton:\n",
    "    if pos in ['NN', 'NNP', 'NNS', 'NNPS'] :\n",
    "        all_nouns_clinton.append(word)\n",
    "print(all_nouns_clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokens_clinton = [lmtzr.lemmatize(word) for word in all_nouns_clinton]\n",
    "tokens_clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#frequency distribution and top 20 nouns \n",
    "freq_clinton_nouns = nltk.FreqDist(tokens_clinton)\n",
    "top_common_nouns_clinton = freq_clinton_nouns.most_common(20)\n",
    "top_common_nouns_clinton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filtering only verbs\n",
    "all_verbs_clinton= []\n",
    "for word, pos in tagged_sent_clinton:\n",
    "    if pos in ['VB', 'VBG', 'VBN', 'VBP','VBZ'] :\n",
    "        all_verbs_clinton.append(word)\n",
    "print(all_verbs_clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lemmatize\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokens_clinton_verbs = [lmtzr.lemmatize(word) for word in all_verbs_clinton]\n",
    "tokens_clinton_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#frequency distribution and top 20 nouns \n",
    "freq_clinton_verbs = nltk.FreqDist(tokens_clinton_verbs)\n",
    "top_common_verbs_clinton = freq_clinton_verbs.most_common(20)\n",
    "top_common_verbs_clinton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# First Debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the TEXT columns as list\n",
    "first_text_list = list(first_debate.TEXT)\n",
    "print(first_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the file\n",
    "with open('first_text_list.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for item in first_text_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using list comprehensions, lowercase the list\n",
    "first_text = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in first_debate['TEXT'])\n",
    "#convert text from list to string\n",
    "first_text = ''.join(first_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of punctuation\n",
    "first_text = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\''\\``]\", \" \", first_text).split())\n",
    "#remove stock market tickers like $GE\n",
    "first_text = ' '.join(re.sub(\"r'\\$\\W+\\w*\", \" \", first_text).split())\n",
    "#replace consecutive non-ASCII characters with a space\n",
    "first_text = ' '.join(re.sub(r\"[^\\x00-\\x7F]+\", \" \", first_text).split())\n",
    "#removal of numbers\n",
    "first_text = ' '.join(re.sub(r\"(\\d+)\", \" \", first_text).split())\n",
    "first_text = first_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_tokens = nltk.word_tokenize(first_text)\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "first_tokens = [token for token in first_tokens if token not in stop]\n",
    "\n",
    "# remove words less than two letters\n",
    "first_tokens = [word for word in first_tokens if len(word) >= 2]\n",
    "\n",
    "preprocessed_first= ' '.join(first_tokens)\n",
    "preprocessed_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('firstdebatecleaned.txt','w') as f:\n",
    "    f.write(preprocessed_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's now visualize the 10 most frequent words without stopwords\n",
    "freq_dist_first = nltk.FreqDist(first_tokens)\n",
    "freq_dist_first.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS_tag in NLTK\n",
    "tagged_sent_first = nltk.pos_tag(first_tokens)\n",
    "#Regular expression outputting only nouns as list \n",
    "all_nouns_first= []\n",
    "for word, pos in tagged_sent_first:\n",
    "    if pos in ['NN', 'NNP', 'NNS', 'NNPS'] :\n",
    "        all_nouns_first.append(word)\n",
    "print(all_nouns_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize the nouns \n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokensN_first = [lmtzr.lemmatize(word) for word in all_nouns_first]\n",
    "#nouns frequencies\n",
    "freq_nouns_first = nltk.FreqDist(tokensN_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_common_nouns_first = freq_nouns_first.most_common(20)\n",
    "top_common_nouns_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS_tag in NLTK\n",
    "tagged_sent_first = nltk.pos_tag(first_tokens)\n",
    "#Regular expression outputting only nouns as list \n",
    "all_verbs_first= []\n",
    "for word, pos in tagged_sent_first:\n",
    "    if pos in ['VB', 'VBG', 'VBN', 'VBP','VBZ']:\n",
    "        all_verbs_first.append(word)\n",
    "print(all_verbs_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize the nouns \n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokensN_first = [lmtzr.lemmatize(word) for word in all_verbs_first]\n",
    "#nouns frequencies\n",
    "freq_verbs_first = nltk.FreqDist(tokensN_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_common_verbs_first = freq_verbs_first.most_common(20)\n",
    "top_common_verbs_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Cos Similarity between candidates' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counterA = Counter(preprocessed_first.split())\n",
    "counterB = Counter(preprocessed_tweets.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counterA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counterB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import dropwhile\n",
    "# Reiterate over key_count for different word frequency thresholds\n",
    "for key, count in dropwhile(lambda key_count: key_count[1] > 50, counterB.most_common()):\n",
    "    del counterB[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(counterB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets50 = ' '.join(counterB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "documents = [tweets50, preprocessed_first] \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Document Term Matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "sparse_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "tfidf = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=tfidf_vectorizer.get_feature_names(), \n",
    "                  )\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(tfidf, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Cosine with BOWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.matutils import softcossim \n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "print(gensim.__version__)\n",
    "\n",
    "# Download the FastText model\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dictionary and a corpus.\n",
    "documents = [tweets50.split(), preprocessed_first.split()]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "debate_vec = dictionary.doc2bow(simple_preprocess(preprocessed_first))\n",
    "tweets_vec = dictionary.doc2bow(simple_preprocess(tweets50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute soft cosine similarity\n",
    "print(softcossim(debate_vec, tweets_vec, similarity_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
