{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import os\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Luca Nannini\\Desktop/AllDebates.csv')\n",
    "df.set_index(\"TURN STARTS\", inplace= True)\n",
    "first_debate = df.iloc[:131]\n",
    "#first debate text:\n",
    "debate = list(first_debate.TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Debate Overall LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debtok = [\n",
    "     [word for word in document.lower().split()]\n",
    "    for document in debate\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       debtok]\n",
    "# remove words less than two letters\n",
    "debate = [list(filter(None, [re.sub(r'\\W*\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       debate]\n",
    "# remove punctuation\n",
    "debate = [list(filter(None, [re.sub(\"[\\.\\,\\…\\!\\?\\:\\;\\-\\—\\_\\=\\*\\@\\#\\$\\\"\\''\\``]\",'', x) for x in y]))\n",
    "                                    for y in debate]\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist = ['people','want','know','believe','you','really','lot','tell','young','stopandfrisk','that','get','got','much','many','put','kind','thanks','thank','think','well','nobody','take','taken','taking','going','go','things','maybe','something','yes','way','would','could','actually','almost','see','seen','sean','called','thing','let','done','went','say','whether','said','look','one','like','also','good','new','ever','little','cannot','everything','lester','even','hannity'] + list(stoplist)\n",
    "\n",
    "cleaned_debate = [\n",
    "     [word for word in document if word not in stoplist]\n",
    "    for document in debate\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(cleaned_debate)\n",
    "corpus = [dictionary.doc2bow(text) for text in cleaned_debate]\n",
    "\n",
    "total_topics = 5\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics, passes=50, per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Debate Semantic Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n",
    "#data_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda = pd.DataFrame(data_lda)\n",
    "print(df_lda.shape)\n",
    "df_lda = df_lda.fillna(0).T\n",
    "print(df_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "g=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine', linewidths=1, figsize=(10, 12))\n",
    "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "plt.show()\n",
    "#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Debate pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(lda, corpus, dictionary, mds='TSNE')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Debate Topic WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2000,\n",
    "                  height=1400,\n",
    "                  max_words=15,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(5, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=200)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Debate Word Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model = FastText(cleaned_debate, size=100, window=50, min_count=10, workers=6)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA analysis of each segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Achieving Prosperity: jobs creation, bringing back expatriated American manufacturers, tax policy.\n",
    "- Candidates private scandals: Trump’s tax return release, Clinton’s e-mails scandal.\n",
    "- America’s Direction: healing race relations, police bias, Trump’s questioning Obama’s birth certificate legitimacy.\n",
    "- Securing America: national institutions cyber attacks, ISIS, homegrown terroristic attacks, Iraq War, nuclear weapons policy.\n",
    "- Mutual Acceptance & Election Outcome: Trump’s opinion on Clinton’s public figure, acceptance of election outcome.\n",
    " ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_I = list(first_debate.loc['09:04:52':'09:30:11'].TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_I = [\n",
    "     [word for word in document.lower().split()]\n",
    "    for document in Deb_I\n",
    " ]\n",
    "\n",
    "Deb_I = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       Deb_I]\n",
    "# remove words less than two letters\n",
    "Deb_I = [list(filter(None, [re.sub(r'\\W*\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       Deb_I]\n",
    "# remove punctuation\n",
    "Deb_I = [list(filter(None, [re.sub(\"[\\.\\,\\…\\!\\?\\:\\;\\-\\—\\_\\=\\*\\@\\#\\$\\\"\\''\\``]\",'', x) for x in y]))\n",
    "          for y in Deb_I]\n",
    "# remove common words and tokenize\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist = ['know','believe','you','really','lot','that','get','got','much','many','put','kind','thanks','thank','think','well','take','taken','going','go','things','maybe','something','yes','way','would','could','actually','almost','see','sean','called','thing','let','done','went','say','whether','said','look','one','like','also','good','new','ever','little','cannot','everything','lester','even','hannity'] + list(stoplist)\n",
    "Deb_I = [\n",
    "     [word for word in document if word not in stoplist]\n",
    "    for document in Deb_I\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaryI = corpora.Dictionary(Deb_I)\n",
    "corpusI = [dictionaryI.doc2bow(text) for text in Deb_I]\n",
    "\n",
    "total_topics = 3\n",
    "lda_I = models.LdaModel(corpusI, id2word=dictionaryI, num_topics=total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_I.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_II = list(first_debate.loc['09:31:38':'09:41:42'].TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_II = [\n",
    "     [word for word in document.lower().split()]\n",
    "    for document in Deb_II\n",
    " ]\n",
    "Deb_II = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       Deb_II]\n",
    "# remove words less than two letters\n",
    "Deb_II = [list(filter(None, [re.sub(r'\\W*\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       Deb_II]\n",
    "# remove punctuation\n",
    "Deb_II = [list(filter(None, [re.sub(\"[\\.\\,\\…\\!\\?\\:\\;\\-\\—\\_\\=\\*\\@\\#\\$\\\"\\''\\``]\",'', x) for x in y]))\n",
    "          for y in Deb_II]\n",
    "# remove common words and tokenize\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist = ['know','believe','you','really','lot','that','get','got','much','many','put','kind','thanks','thank','think','well','take','taken','going','go','things','maybe','something','yes','way','would','could','actually','almost','see','sean','called','thing','let','done','went','say','whether','said','look','one','like','also','good','new','ever','little','cannot','everything','lester','even','hannity'] + list(stoplist)\n",
    "Deb_II = [\n",
    "     [word for word in document if word not in stoplist]\n",
    "    for document in Deb_II\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionaryII = corpora.Dictionary(Deb_II)\n",
    "corpusII = [dictionaryII.doc2bow(text) for text in Deb_II]\n",
    "\n",
    "total_topics = 3\n",
    "lda_II = models.LdaModel(corpusII, id2word=dictionaryII, num_topics=total_topics)\n",
    "\n",
    "\n",
    "#IF: (IndexError: index 391 is out of bounds for axis 1 with size 386) THEN shut down and restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_II.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_III = list(first_debate.loc['09:44:06':'10:04:10'].TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_III = [\n",
    "     [word for word in document.lower().split()]\n",
    "    for document in Deb_III\n",
    " ]\n",
    "Deb_III = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       Deb_III]\n",
    "# remove words less than two letters\n",
    "Deb_III = [list(filter(None, [re.sub(r'\\W*\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       Deb_III]\n",
    "# remove punctuation\n",
    "Deb_III = [list(filter(None, [re.sub(\"[\\.\\,\\…\\!\\?\\:\\;\\-\\—\\_\\=\\*\\@\\#\\$\\\"\\''\\``]\",'', x) for x in y]))\n",
    "          for y in Deb_III]\n",
    "# remove common words and tokenize\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist = ['know','believe','you','really','lot','that','get','got','much','many','put','kind','thanks','thank','think','well','take','taken','going','go','things','maybe','something','yes','way','would','could','actually','almost','see','sean','called','thing','let','done','went','say','whether','said','look','one','like','also','good','new','ever','little','cannot','everything','lester','even','hannity'] + list(stoplist)\n",
    "Deb_III = [\n",
    "     [word for word in document if word not in stoplist]\n",
    "    for document in Deb_III\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaryIII = corpora.Dictionary(Deb_III)\n",
    "corpusIII = [dictionaryIII.doc2bow(text) for text in Deb_III]\n",
    "\n",
    "total_topics = 3\n",
    "lda_III = models.LdaModel(corpusIII, id2word=dictionaryIII, num_topics=total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_III.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_IV = list(first_debate.loc['10:06:26':'10:31:51'].TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_IV = [\n",
    "     [word for word in document.lower().split()]\n",
    "    for document in Deb_IV\n",
    " ]\n",
    "Deb_IV = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       Deb_IV]\n",
    "# remove words less than two letters\n",
    "Deb_IV = [list(filter(None, [re.sub(r'\\W*\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       Deb_IV]\n",
    "# remove punctuation\n",
    "Deb_IV = [list(filter(None, [re.sub(\"[\\.\\,\\…\\!\\?\\:\\;\\-\\—\\_\\=\\*\\@\\#\\$\\\"\\''\\``]\",'', x) for x in y]))\n",
    "          for y in Deb_IV]\n",
    "# remove common words and tokenize\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist = ['know','believe','you','really','lot','that','get','got','much','many','put','kind','thanks','thank','think','well','take','taken','going','go','things','maybe','something','yes','way','would','could','actually','almost','see','sean','called','thing','let','done','went','say','whether','said','look','one','like','also','good','new','ever','little','cannot','everything','lester','even','hannity'] + list(stoplist)\n",
    "Deb_IV = [\n",
    "     [word for word in document if word not in stoplist]\n",
    "    for document in Deb_IV\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionaryIV = corpora.Dictionary(Deb_IV)\n",
    "corpusIV = [dictionaryIV.doc2bow(text) for text in Deb_IV]\n",
    "\n",
    "total_topics = 3\n",
    "lda_IV = models.LdaModel(corpusIV, id2word=dictionaryIV, num_topics=total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_IV.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_V = list(first_debate.loc['10:33:04':'10:37:43'].TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deb_V = [\n",
    "     [word for word in document.lower().split()]\n",
    "    for document in Deb_V\n",
    " ]\n",
    "Deb_V = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       Deb_V]\n",
    "# remove words less than two letters\n",
    "Deb_V = [list(filter(None, [re.sub(r'\\W*\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       Deb_V]\n",
    "# remove punctuation\n",
    "Deb_V = [list(filter(None, [re.sub(\"[\\.\\,\\…\\!\\?\\:\\;\\-\\—\\_\\=\\*\\@\\#\\$\\\"\\''\\``]\",'', x) for x in y]))\n",
    "          for y in Deb_V]\n",
    "# remove common words and tokenize\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist = ['know','believe','you','really','lot','that','get','got','much','many','put','kind','thanks','thank','think','well','take','taken','going','go','things','maybe','something','yes','way','would','could','actually','almost','see','sean','called','thing','let','done','went','say','whether','said','look','one','like','also','good','new','ever','little','cannot','everything','lester','even','hannity'] + list(stoplist)\n",
    "Deb_V = [\n",
    "     [word for word in document if word not in stoplist]\n",
    "    for document in Deb_V\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionaryV = corpora.Dictionary(Deb_V)\n",
    "corpusV = [dictionaryV.doc2bow(text) for text in Deb_V]\n",
    "\n",
    "total_topics = 2\n",
    "lda_V = models.LdaModel(corpusV, id2word=dictionaryV, num_topics=total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_V.show_topics(total_topics,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
