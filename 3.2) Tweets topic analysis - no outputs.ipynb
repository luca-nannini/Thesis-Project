{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import os\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time conversion of Tweets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(r'C:\\Users\\Luca Nannini\\Desktop/TweetsClean.csv')\n",
    "tweet = tweets[tweets['RT'] == False]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "clean_timestamp =  tweet['created_at'].apply(lambda x: datetime.strptime(x,'%Y-%m-%dT%H:%M:%SZ'))\n",
    "#offset in hours for EST timezone\n",
    "offset_hours = -16\n",
    "#account for offset from UTC using timedelta                                \n",
    "local_timestamp = clean_timestamp + timedelta(hours=offset_hours)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_timestamp = local_timestamp.dt.strftime('%X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with the new time format and tweets text\n",
    "TIME = final_timestamp\n",
    "TEXT = tweet['text']\n",
    "new_tweet = pd.DataFrame(dict(TIME = TIME, TEXT = TEXT))\n",
    "new_tweet.set_index(\"TIME\", inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall tweet volume & tweets LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Volume graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NB. In order to visualize the tweet volume, the TIME column must be moved away as index. \n",
    "BUT for sub-chunk the new_tweet dataframe function should not be run.\n",
    "Then, for the following segments analysis section do RUN AGAIN the entire time conversion section obtaining again the original \"new_tweet\" dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweet.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_ratio = pd.to_datetime(new_tweet['TIME'], format='%X')\n",
    "\n",
    "trace = go.Histogram(\n",
    "    x=tweets_ratio,\n",
    "    marker=dict(\n",
    "        color='lightblue'\n",
    "    ),\n",
    "    opacity=0.75\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Overall Debate',\n",
    "    height=450,\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        title='Time Segment for each bar = 29sec'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Tweet Volume'\n",
    "    ),\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(new_tweet.TEXT)\n",
    "\n",
    "twtok = [\n",
    "     [word for word in document.lower().split()]\n",
    "      for document in tweets\n",
    " ]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in twtok:\n",
    "     for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "debtok = [\n",
    "     [token for token in text if frequency[token] > 1]\n",
    "     for text in twtok\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [list(filter(None, [re.sub(r'\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       debtok]\n",
    "tweets = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(@[A-Za-z0-9]+)','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[\\.\\,\\'\\\"\\!\\?\\:\\;\\-\\_\\/\\=\\(\\)\\|\\*\\&\\@\\#\\$\\\"]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[^\\x00-\\x7F]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(.)\\1{2,}',r'\\1', x) for x in y])) for y in\n",
    "       tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist1 = ['hillary','clinton','trump','donald','debatenight','debate','people','want','know','believe','you','really','lot','tell','young','stopandfrisk','that','get','got','much','many','put','kind','thanks','thank','think','well','nobody','take','taken','taking','going','go','things','maybe','something','yes','way','would','could','actually','almost','see','seen','sean','called','thing','let','done','went','say','whether','said','look','one','like','also','good','new','ever','little','cannot','everything','lester','even','hannity'] + list(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "stoplist = stopwords.words('english')\n",
    "cleaned_tweets = [\n",
    "     [lmtzr.lemmatize(word) for word in document if word not in stoplist1]\n",
    "    for document in tweets\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(cleaned_tweets)\n",
    "corpus = [dictionary.doc2bow(text) for text in cleaned_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = 5\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets Semantic Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n",
    "#data_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda = pd.DataFrame(data_lda)\n",
    "print(df_lda.shape)\n",
    "df_lda = df_lda.fillna(0).T\n",
    "print(df_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "g=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine', linewidths=1, figsize=(10, 12))\n",
    "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "plt.show()\n",
    "#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(lda, corpus, dictionary, mds='TSNE')\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets Debate Topic WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2000,\n",
    "                  height=1400,\n",
    "                  max_words=15,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(5, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=200)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Debate Word Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model = FastText(cleaned_tweets, size=100, window=50, min_count=500, workers=6)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debate & Tweets Segment Time Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tweets segments, +5 sec from the end of the final sentence expressed by a candidate is given before moving to the next one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Achieving Prosperity: jobs creation, bringing back expatriated American manufacturers, tax policy.\n",
    "- Candidates private scandals: Trump’s tax return release, Clinton’s e-mails scandal.\n",
    "- America’s Direction: healing race relations, police bias, Trump’s questioning Obama’s birth certificate legitimacy.\n",
    "- Securing America: national institutions cyber attacks, ISIS, homegrown terroristic attacks, Iraq War, nuclear weapons policy.\n",
    "- Mutual Acceptance & Election Outcome: Trump’s opinion on Clinton’s public figure, acceptance of election outcome.\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Before running this section, RUN AGAIN the ENTIRE time conversion section for obtain the original \"new_tweets\" dataframe having as index the TIME - otherwise these following codes will output list of strings with no tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I° Segment, \"Achieving Prosperity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define \"Achieving Prosperity\" segment\n",
    "TW_I = new_tweet.loc['09:04:53':'09:31:23']\n",
    "TW_I.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW_I_TIME = TW_I['TIME'] \n",
    "\n",
    "tweets_ratio_I = pd.to_datetime(TW_I_TIME, format='%X')\n",
    "\n",
    "trace = go.Histogram(\n",
    "    x=tweets_ratio_I,\n",
    "    marker=dict(\n",
    "        color='lightblue'\n",
    "    ),\n",
    "    opacity=0.75\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='I. Achieving Prosperity',\n",
    "    height=450,\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        title='Time Segment for each bar = 14sec'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Tweet Volume'\n",
    "    ),\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = list(TW_I.TEXT)\n",
    "\n",
    "twtok1 = [\n",
    "     [word for word in document.lower().split()]\n",
    "      for document in tweets1\n",
    " ]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in twtok1:\n",
    "     for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "debtok = [\n",
    "     [token for token in text if frequency[token] > 1]\n",
    "     for text in twtok1\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [list(filter(None, [re.sub(r'\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       debtok]\n",
    "tweets = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(@[A-Za-z0-9]+)','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[\\.\\,\\'\\\"\\!\\?\\:\\;\\-\\_\\/\\=\\(\\)\\|\\*\\@\\#\\$\\\"]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[^\\x00-\\x7F]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(.)\\1{2,}',r'\\1', x) for x in y])) for y in\n",
    "       tweets]\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stoplist = stopwords.words('english')\n",
    "cleaned_tweets1 = [\n",
    "     [lmtzr.lemmatize(word) for word in document if word not in stoplist1]\n",
    "    for document in tweets\n",
    " ]\n",
    "\n",
    "cleaned_tweets1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary1 = corpora.Dictionary(cleaned_tweets1)\n",
    "\n",
    "corpus1 = [dictionary1.doc2bow(text) for text in cleaned_tweets1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = 3\n",
    "lda = models.LdaModel(corpus1, id2word=dictionary1, num_topics=total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II° Segment, \"Candidate Figure Issues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define \"Candidate Figure Issues\" segment\n",
    "TW_II = new_tweet.loc['09:31:38':'09:43:41']\n",
    "TW_II.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW_II_TIME = TW_II['TIME'] \n",
    "\n",
    "tweets_ratio_II = pd.to_datetime(TW_II_TIME, format='%X')\n",
    "\n",
    "trace = go.Histogram(\n",
    "    x=tweets_ratio_II,\n",
    "    marker=dict(\n",
    "        color='lightblue'\n",
    "    ),\n",
    "    opacity=0.75\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='II. Candidate Figure Issues',\n",
    "    height=450,\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        title='Time Segment for each bar = 9sec'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Tweet Volume'\n",
    "    ),\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2 = list(TW_II.TEXT)\n",
    "\n",
    "twtok2 = [\n",
    "     [word for word in document.lower().split()]\n",
    "      for document in tweets2\n",
    " ]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in twtok2:\n",
    "     for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "debtok2 = [\n",
    "     [token for token in text if frequency[token] > 1]\n",
    "     for text in twtok2\n",
    " ]\n",
    "\n",
    "tweets = [list(filter(None, [re.sub(r'\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       debtok2]\n",
    "tweets = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(@[A-Za-z0-9]+)','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[\\.\\,\\'\\\"\\!\\?\\:\\;\\_\\-\\/\\=\\(\\)\\|\\*\\@\\&\\#\\$\\\"]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[^\\x00-\\x7F]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(.)\\1{2,}',r'\\1', x) for x in y])) for y in\n",
    "       tweets]\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "cleaned_tweets2 = [\n",
    "     [lmtzr.lemmatize(word) for word in document if word not in stoplist1]\n",
    "    for document in tweets\n",
    " ]\n",
    "\n",
    "cleaned_tweets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2 = corpora.Dictionary(cleaned_tweets2)\n",
    "corpus2 = [dictionary2.doc2bow(text) for text in cleaned_tweets2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = 3\n",
    "lda2 = models.LdaModel(corpus2, id2word=dictionary2, num_topics=total_topics)\n",
    "\n",
    "lda2.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III° Segment, \"America's Direction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define \"America's direction\" segment\n",
    "TW_III = new_tweet.loc['09:44:06':'10:06:14']\n",
    "TW_III.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW_III_TIME = TW_III['TIME'] \n",
    "\n",
    "tweets_ratio_III = pd.to_datetime(TW_III_TIME, format='%X')\n",
    "\n",
    "trace = go.Histogram(\n",
    "    x=tweets_ratio_III,\n",
    "    marker=dict(\n",
    "        color='lightblue'\n",
    "    ),\n",
    "    opacity=0.75\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"III. America's Direction\",\n",
    "    height=450,\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        title='Time Segment for each bar = 14sec'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Tweet Volume'\n",
    "    ),\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets3 = list(TW_III.TEXT)\n",
    "\n",
    "twtok3 = [\n",
    "     [word for word in document.lower().split()]\n",
    "      for document in tweets3\n",
    " ]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in twtok3:\n",
    "     for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "debtok2 = [\n",
    "     [token for token in text if frequency[token] > 1]\n",
    "     for text in twtok3\n",
    " ]\n",
    "\n",
    "tweets = [list(filter(None, [re.sub(r'\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       debtok2]\n",
    "tweets = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(@[A-Za-z0-9]+)','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[\\.\\,\\'\\\"\\!\\?\\:\\;\\-\\_\\/\\=\\(\\)\\|\\*\\&\\@\\#\\$\\\"]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[^\\x00-\\x7F]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(.)\\1{2,}',r'\\1', x) for x in y])) for y in\n",
    "       tweets]\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "cleaned_tweets3 = [\n",
    "     [lmtzr.lemmatize(word) for word in document if word not in stoplist1]\n",
    "    for document in tweets\n",
    " ]\n",
    "\n",
    "cleaned_tweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary3 = corpora.Dictionary(cleaned_tweets3)\n",
    "corpus3 = [dictionary3.doc2bow(text) for text in cleaned_tweets3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = 3\n",
    "lda3 = models.LdaModel(corpus3, id2word=dictionary3, num_topics=total_topics)\n",
    "\n",
    "lda3.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV° Segment, \"Securing America\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define \"Securing America\" segment\n",
    "TW_IV = new_tweet.loc['10:06:26':'10:33:00']\n",
    "TW_IV.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TW_IV_TIME = TW_IV['TIME'] \n",
    "\n",
    "tweets_ratio_IV = pd.to_datetime(TW_IV_TIME, format='%X')\n",
    "\n",
    "trace = go.Histogram(\n",
    "    x=tweets_ratio_IV,\n",
    "    marker=dict(\n",
    "        color='lightblue'\n",
    "    ),\n",
    "    opacity=0.75\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='IV. Securing America',\n",
    "    height=450,\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        title='Time Segment for each bar = 14sec'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Tweet Volume'\n",
    "    ),\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets4 = list(TW_IV.TEXT)\n",
    "\n",
    "twtok4 = [\n",
    "     [word for word in document.lower().split()]\n",
    "      for document in tweets4\n",
    " ]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in twtok4:\n",
    "     for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "debtok4 = [\n",
    "     [token for token in text if frequency[token] > 1]\n",
    "     for text in twtok4\n",
    " ]\n",
    "\n",
    "tweets = [list(filter(None, [re.sub(r'\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       debtok4]\n",
    "tweets = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(@[A-Za-z0-9]+)','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[\\.\\,\\'\\\"\\!\\?\\:\\;\\-\\_\\/\\=\\(\\)\\|\\*\\&\\@\\#\\$\\\"]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[^\\x00-\\x7F]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(.)\\1{2,}',r'\\1', x) for x in y])) for y in\n",
    "       tweets]\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "cleaned_tweets4 = [\n",
    "     [lmtzr.lemmatize(word) for word in document if word not in stoplist1]\n",
    "    for document in tweets\n",
    " ]\n",
    "\n",
    "cleaned_tweets4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary4 = corpora.Dictionary(cleaned_tweets4)\n",
    "corpus4 = [dictionary4.doc2bow(text) for text in cleaned_tweets4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = 3\n",
    "lda4 = models.LdaModel(corpus4, id2word=dictionary4, num_topics=total_topics)\n",
    "\n",
    "lda4.show_topics(total_topics,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V° Segment, \"Mutual & Election Acceptance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define \"Mutual and Election Acceptance\"\n",
    "TW_V = new_tweet.loc['10:33:04':'10:38:56']\n",
    "TW_V.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TW_V_TIME = TW_V['TIME'] \n",
    "\n",
    "tweets_ratio_V = pd.to_datetime(TW_V_TIME, format='%X')\n",
    "\n",
    "trace = go.Histogram(\n",
    "    x=tweets_ratio_V,\n",
    "    marker=dict(\n",
    "        color='lightblue'\n",
    "    ),\n",
    "    opacity=0.75\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='V. Mutual and Election Acceptance',\n",
    "    height=450,\n",
    "    width=1200,\n",
    "    xaxis=dict(\n",
    "        title='Time Segment for each bar = 9sec'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Tweet Volume'\n",
    "    ),\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets5 = list(TW_V.TEXT)\n",
    "\n",
    "twtok5 = [\n",
    "     [word for word in document.lower().split()]\n",
    "      for document in tweets5\n",
    " ]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in twtok5:\n",
    "     for token in text:\n",
    "        frequency[token] += 1\n",
    "        \n",
    "debtok5 = [\n",
    "     [token for token in text if frequency[token] > 1]\n",
    "     for text in twtok5\n",
    " ]\n",
    "\n",
    "tweets = [list(filter(None, [re.sub(r'\\b\\w{1,2}\\b','', x) for x in y])) for y in\n",
    "       debtok5]\n",
    "tweets = [list(filter(None, [re.sub(r'\\d+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(@[A-Za-z0-9]+)','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[\\.\\,\\'\\\"\\!\\?\\:\\;\\-\\_\\/\\=\\(\\)\\|\\*\\@\\#\\$\\\"]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'[^\\x00-\\x7F]+','', x) for x in y])) for y in\n",
    "       tweets]\n",
    "tweets = [list(filter(None, [re.sub(r'(.)\\1{2,}',r'\\1', x) for x in y])) for y in\n",
    "       tweets]\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "cleaned_tweets5 = [\n",
    "     [lmtzr.lemmatize(word) for word in document if word not in stoplist]\n",
    "    for document in tweets\n",
    " ]\n",
    "\n",
    "cleaned_tweets5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary5 = corpora.Dictionary(cleaned_tweets5)\n",
    "corpus5 = [dictionary5.doc2bow(text) for text in cleaned_tweets5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topics = 2\n",
    "lda5 = models.LdaModel(corpus5, id2word=dictionary5, num_topics=total_topics)\n",
    "\n",
    "lda5.show_topics(total_topics,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
