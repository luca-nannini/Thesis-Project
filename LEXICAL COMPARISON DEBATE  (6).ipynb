{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Comparison on Transcript and Tweets produced during the First 16' US Presidential Debate\n",
    "\n",
    "_Luca Nannini_\n",
    "\n",
    "- __Preparing the raw .text files of the official debate transcription__\n",
    "\n",
    "- __Preparing the .csv file containing only the original tweets - no retweets - about the event__ \n",
    "\n",
    "- __Preparing a comparative analysis of lexical intersection between the datasets__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index \n",
    "\n",
    "[1. Cleaning the Transcripts](#1.-Cleaning-the-Transcripts)\n",
    "> [1.1 Clinton's Speech](#1.1-Clinton's-Speech)\n",
    "\n",
    "> [1.2 Trump's Speech](#1.2-Trump's-Speech)\n",
    "\n",
    "> [1.3 Cleaned Debate Transcript Datasets](#1.3-Cleaned-Debate-Transcript-Datasets)\n",
    "\n",
    "[2. Cleaning Tweets Dataset](#2.-Cleaning-Tweets-Dataset) \n",
    "\n",
    "[3. Comparative Analysis of Lexical Intersection](#3.-Comparative-Analysis-of-Lexical-Intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "_General Guidelines (NLTK)_:\n",
    "1. Load the raw text.\n",
    "1. Split into tokens.\n",
    "1. Convert to lowercase.\n",
    "1. Remove punctuation from each token.\n",
    "1. Filter out remaining tokens that are not alphabetic.\n",
    "1. Filter out tokens that are stop words.\n",
    "1. Stemming the tokens.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning the Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Clinton's Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import sklearn\n",
    "import re\n",
    "\n",
    "Clinton= r'C:\\Users\\Luca Nannini\\Desktop\\Thesis/CLINTON_DEBATE.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clinton = open(Clinton, 'r+')\n",
    "text = Clinton.read()\n",
    "Clinton.close()\n",
    "wordsC = text.split()\n",
    "wordsC = re.split(r'\\W+', text)\n",
    "print(wordsC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsC = [word.lower() for word in wordsC]\n",
    "print(wordsC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in wordsC]\n",
    "print(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsC = [word for word in stripped if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "wordsC = [w for w in wordsC if not w in stop_words]\n",
    "print(wordsC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in wordsC]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordsC = list(set(wordsC))\n",
    "from collections import OrderedDict\n",
    "OrderedDict((x, True) for x in wordsC).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordsC.remove('clinton')\n",
    "wordsC.remove('hillaryclinton')\n",
    "print(wordsC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Trump's Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trump= r'C:\\Users\\Luca Nannini\\Desktop\\Thesis/TRUMP_DEBATE.txt'\n",
    "\n",
    "Trump = open(Trump, 'r+')\n",
    "text = Trump.read()\n",
    "Trump.close()\n",
    "wordsT =text.split()\n",
    "print(wordsT)\n",
    "\n",
    "import re\n",
    "wordsT = re.split(r'\\W+', text)\n",
    "print(wordsT)\n",
    "\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in wordsT]\n",
    "print(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsT = [word for word in stripped if word.isalpha()]\n",
    "wordsT = [word.lower() for word in wordsT]\n",
    "print(wordsT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "wordsT = [w for w in wordsT if not w in stop_words]\n",
    "print(wordsT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in wordsT]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordsT = list(set(wordsT))\n",
    "from collections import OrderedDict\n",
    "OrderedDict((x, True) for x in wordsT).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordsT.remove('trump')\n",
    "print(wordsT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cleaned Debate Transcript Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Trump = print(wordsT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Clinton = print(wordsC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = r'C:\\Users\\Luca Nannini\\Desktop\\Thesis/TWEETS.txt'\n",
    "\n",
    "Tweets = open(Tweets, 'r+')\n",
    "text = Tweets.read()\n",
    "Tweets.close()\n",
    "Tw = text.split()\n",
    "print(Tw)\n",
    "\n",
    "import re\n",
    "Tw = re.split(r'\\W+', text)\n",
    "print(Tw)\n",
    "\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in Tw]\n",
    "print(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tw = [word for word in stripped if word.isalpha()]\n",
    "Tw = [word.lower() for word in Tw]\n",
    "print(Tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "Tw = [w for w in Tw if not w in stop_words]\n",
    "print(Tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in Tw]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = list(set(Tw))\n",
    "from collections import OrderedDict\n",
    "OrderedDict((x, True) for x in Tw).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparative Analysis of Lexical Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Transcription = MS = (set().union(wordsT, wordsC))\n",
    "print(MS)\n",
    "#merging in an unique list with no duplicate words Trump & Clinton's personal speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matching_Words = set(MS).intersection(Tweets)\n",
    "print(Matching_Words)\n",
    "#comparison of words matching in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Symmetric_Word_Difference = set(MS).symmetric_difference(set(Tweets))\n",
    "print(Symmetric_Word_Difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Matching_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Symmetric_Word_Difference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
